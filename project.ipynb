{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "  \n",
    "# fetch dataset \n",
    "drug_consumption_quantified = fetch_ucirepo(id=373) \n",
    "\n",
    "# data (as pandas dataframes) \n",
    "X = drug_consumption_quantified.data.features \n",
    "y = drug_consumption_quantified.data.targets \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>education</th>\n",
       "      <th>country</th>\n",
       "      <th>ethnicity</th>\n",
       "      <th>nscore</th>\n",
       "      <th>escore</th>\n",
       "      <th>oscore</th>\n",
       "      <th>ascore</th>\n",
       "      <th>cscore</th>\n",
       "      <th>impuslive</th>\n",
       "      <th>ss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1885.00000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.03461</td>\n",
       "      <td>-0.000256</td>\n",
       "      <td>-0.003806</td>\n",
       "      <td>0.355542</td>\n",
       "      <td>-0.309577</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>-0.000163</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>-0.000245</td>\n",
       "      <td>-0.000386</td>\n",
       "      <td>0.007216</td>\n",
       "      <td>-0.003292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.87836</td>\n",
       "      <td>0.482588</td>\n",
       "      <td>0.950078</td>\n",
       "      <td>0.700335</td>\n",
       "      <td>0.166226</td>\n",
       "      <td>0.998106</td>\n",
       "      <td>0.997448</td>\n",
       "      <td>0.996229</td>\n",
       "      <td>0.997440</td>\n",
       "      <td>0.997523</td>\n",
       "      <td>0.954435</td>\n",
       "      <td>0.963701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.482460</td>\n",
       "      <td>-2.435910</td>\n",
       "      <td>-0.570090</td>\n",
       "      <td>-1.107020</td>\n",
       "      <td>-3.464360</td>\n",
       "      <td>-3.273930</td>\n",
       "      <td>-3.273930</td>\n",
       "      <td>-3.464360</td>\n",
       "      <td>-3.464360</td>\n",
       "      <td>-2.555240</td>\n",
       "      <td>-2.078480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.95197</td>\n",
       "      <td>-0.482460</td>\n",
       "      <td>-0.611130</td>\n",
       "      <td>-0.570090</td>\n",
       "      <td>-0.316850</td>\n",
       "      <td>-0.678250</td>\n",
       "      <td>-0.695090</td>\n",
       "      <td>-0.717270</td>\n",
       "      <td>-0.606330</td>\n",
       "      <td>-0.652530</td>\n",
       "      <td>-0.711260</td>\n",
       "      <td>-0.525930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.07854</td>\n",
       "      <td>-0.482460</td>\n",
       "      <td>-0.059210</td>\n",
       "      <td>0.960820</td>\n",
       "      <td>-0.316850</td>\n",
       "      <td>0.042570</td>\n",
       "      <td>0.003320</td>\n",
       "      <td>-0.019280</td>\n",
       "      <td>-0.017290</td>\n",
       "      <td>-0.006650</td>\n",
       "      <td>-0.217120</td>\n",
       "      <td>0.079870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.49788</td>\n",
       "      <td>0.482460</td>\n",
       "      <td>0.454680</td>\n",
       "      <td>0.960820</td>\n",
       "      <td>-0.316850</td>\n",
       "      <td>0.629670</td>\n",
       "      <td>0.637790</td>\n",
       "      <td>0.723300</td>\n",
       "      <td>0.760960</td>\n",
       "      <td>0.584890</td>\n",
       "      <td>0.529750</td>\n",
       "      <td>0.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.59171</td>\n",
       "      <td>0.482460</td>\n",
       "      <td>1.984370</td>\n",
       "      <td>0.960820</td>\n",
       "      <td>1.907250</td>\n",
       "      <td>3.273930</td>\n",
       "      <td>3.273930</td>\n",
       "      <td>2.901610</td>\n",
       "      <td>3.464360</td>\n",
       "      <td>3.464360</td>\n",
       "      <td>2.901610</td>\n",
       "      <td>1.921730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age       gender    education      country    ethnicity  \\\n",
       "count  1885.00000  1885.000000  1885.000000  1885.000000  1885.000000   \n",
       "mean      0.03461    -0.000256    -0.003806     0.355542    -0.309577   \n",
       "std       0.87836     0.482588     0.950078     0.700335     0.166226   \n",
       "min      -0.95197    -0.482460    -2.435910    -0.570090    -1.107020   \n",
       "25%      -0.95197    -0.482460    -0.611130    -0.570090    -0.316850   \n",
       "50%      -0.07854    -0.482460    -0.059210     0.960820    -0.316850   \n",
       "75%       0.49788     0.482460     0.454680     0.960820    -0.316850   \n",
       "max       2.59171     0.482460     1.984370     0.960820     1.907250   \n",
       "\n",
       "            nscore       escore       oscore       ascore       cscore  \\\n",
       "count  1885.000000  1885.000000  1885.000000  1885.000000  1885.000000   \n",
       "mean      0.000047    -0.000163    -0.000534    -0.000245    -0.000386   \n",
       "std       0.998106     0.997448     0.996229     0.997440     0.997523   \n",
       "min      -3.464360    -3.273930    -3.273930    -3.464360    -3.464360   \n",
       "25%      -0.678250    -0.695090    -0.717270    -0.606330    -0.652530   \n",
       "50%       0.042570     0.003320    -0.019280    -0.017290    -0.006650   \n",
       "75%       0.629670     0.637790     0.723300     0.760960     0.584890   \n",
       "max       3.273930     3.273930     2.901610     3.464360     3.464360   \n",
       "\n",
       "         impuslive           ss  \n",
       "count  1885.000000  1885.000000  \n",
       "mean      0.007216    -0.003292  \n",
       "std       0.954435     0.963701  \n",
       "min      -2.555240    -2.078480  \n",
       "25%      -0.711260    -0.525930  \n",
       "50%      -0.217120     0.079870  \n",
       "75%       0.529750     0.765400  \n",
       "max       2.901610     1.921730  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>amphet</th>\n",
       "      <th>amyl</th>\n",
       "      <th>benzos</th>\n",
       "      <th>caff</th>\n",
       "      <th>cannabis</th>\n",
       "      <th>choc</th>\n",
       "      <th>coke</th>\n",
       "      <th>crack</th>\n",
       "      <th>ecstasy</th>\n",
       "      <th>heroin</th>\n",
       "      <th>ketamine</th>\n",
       "      <th>legalh</th>\n",
       "      <th>lsd</th>\n",
       "      <th>meth</th>\n",
       "      <th>mushrooms</th>\n",
       "      <th>nicotine</th>\n",
       "      <th>semer</th>\n",
       "      <th>vsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "      <td>1885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>CL5</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL6</td>\n",
       "      <td>CL0</td>\n",
       "      <td>CL0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>759</td>\n",
       "      <td>976</td>\n",
       "      <td>1305</td>\n",
       "      <td>1000</td>\n",
       "      <td>1385</td>\n",
       "      <td>463</td>\n",
       "      <td>807</td>\n",
       "      <td>1038</td>\n",
       "      <td>1627</td>\n",
       "      <td>1021</td>\n",
       "      <td>1605</td>\n",
       "      <td>1490</td>\n",
       "      <td>1094</td>\n",
       "      <td>1069</td>\n",
       "      <td>1429</td>\n",
       "      <td>982</td>\n",
       "      <td>610</td>\n",
       "      <td>1877</td>\n",
       "      <td>1455</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       alcohol amphet  amyl benzos  caff cannabis  choc  coke crack ecstasy  \\\n",
       "count     1885   1885  1885   1885  1885     1885  1885  1885  1885    1885   \n",
       "unique       7      7     7      7     7        7     7     7     7       7   \n",
       "top        CL5    CL0   CL0    CL0   CL6      CL6   CL6   CL0   CL0     CL0   \n",
       "freq       759    976  1305   1000  1385      463   807  1038  1627    1021   \n",
       "\n",
       "       heroin ketamine legalh   lsd  meth mushrooms nicotine semer   vsa  \n",
       "count    1885     1885   1885  1885  1885      1885     1885  1885  1885  \n",
       "unique      7        7      7     7     7         7        7     5     7  \n",
       "top       CL0      CL0    CL0   CL0   CL0       CL0      CL6   CL0   CL0  \n",
       "freq     1605     1490   1094  1069  1429       982      610  1877  1455  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>amphet</th>\n",
       "      <th>amyl</th>\n",
       "      <th>benzos</th>\n",
       "      <th>caff</th>\n",
       "      <th>cannabis</th>\n",
       "      <th>choc</th>\n",
       "      <th>coke</th>\n",
       "      <th>crack</th>\n",
       "      <th>ecstasy</th>\n",
       "      <th>heroin</th>\n",
       "      <th>ketamine</th>\n",
       "      <th>legalh</th>\n",
       "      <th>lsd</th>\n",
       "      <th>meth</th>\n",
       "      <th>mushrooms</th>\n",
       "      <th>nicotine</th>\n",
       "      <th>semer</th>\n",
       "      <th>vsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1880</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1881</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1882</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1883</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1884</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1885 rows × 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      alcohol  amphet  amyl  benzos  caff  cannabis  choc  coke  crack  \\\n",
       "0           1       1     0       1     1         0     1     0      0   \n",
       "1           1       1     1       0     1         1     1     1      0   \n",
       "2           1       0     0       0     1         1     1     0      0   \n",
       "3           1       0     0       1     1         1     1     1      0   \n",
       "4           1       0     0       0     1         1     1     0      0   \n",
       "...       ...     ...   ...     ...   ...       ...   ...   ...    ...   \n",
       "1880        1       0     0       0     1         1     1     0      0   \n",
       "1881        1       0     0       0     1         1     1     0      0   \n",
       "1882        1       1     1       1     1         1     1     1      0   \n",
       "1883        1       0     0       0     1         1     1     0      0   \n",
       "1884        1       1     0       1     1         1     1     1      0   \n",
       "\n",
       "      ecstasy  heroin  ketamine  legalh  lsd  meth  mushrooms  nicotine  \\\n",
       "0           0       0         0       0    0     0          0         1   \n",
       "1           1       0         1       0    1     1          0         1   \n",
       "2           0       0         0       0    0     0          0         0   \n",
       "3           0       0         1       0    0     0          0         1   \n",
       "4           0       0         0       0    0     0          1         1   \n",
       "...       ...     ...       ...     ...  ...   ...        ...       ...   \n",
       "1880        0       0         0       1    1     0          0         0   \n",
       "1881        1       0         0       1    1     1          1         1   \n",
       "1882        1       0         1       0    1     0          1         1   \n",
       "1883        1       0         0       1    1     0          1         1   \n",
       "1884        1       0         0       1    1     0          1         1   \n",
       "\n",
       "      semer  vsa  \n",
       "0         0    0  \n",
       "1         0    0  \n",
       "2         0    0  \n",
       "3         0    0  \n",
       "4         0    0  \n",
       "...     ...  ...  \n",
       "1880      0    1  \n",
       "1881      0    0  \n",
       "1882      0    0  \n",
       "1883      0    0  \n",
       "1884      0    1  \n",
       "\n",
       "[1885 rows x 19 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_user_values = ['CL0', 'CL1']\n",
    "user_values = ['CL2', 'CL3', 'CL4' 'CL5', 'CL6']\n",
    "\n",
    "y = y.map(lambda value: 0 if value in non_user_values else 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>amphet</th>\n",
       "      <th>amyl</th>\n",
       "      <th>benzos</th>\n",
       "      <th>caff</th>\n",
       "      <th>cannabis</th>\n",
       "      <th>choc</th>\n",
       "      <th>coke</th>\n",
       "      <th>crack</th>\n",
       "      <th>ecstasy</th>\n",
       "      <th>heroin</th>\n",
       "      <th>ketamine</th>\n",
       "      <th>legalh</th>\n",
       "      <th>lsd</th>\n",
       "      <th>meth</th>\n",
       "      <th>mushrooms</th>\n",
       "      <th>nicotine</th>\n",
       "      <th>semer</th>\n",
       "      <th>vsa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "      <td>1885.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.963926</td>\n",
       "      <td>0.360212</td>\n",
       "      <td>0.196286</td>\n",
       "      <td>0.407958</td>\n",
       "      <td>0.980371</td>\n",
       "      <td>0.671088</td>\n",
       "      <td>0.981432</td>\n",
       "      <td>0.364456</td>\n",
       "      <td>0.101326</td>\n",
       "      <td>0.398408</td>\n",
       "      <td>0.112467</td>\n",
       "      <td>0.185676</td>\n",
       "      <td>0.404244</td>\n",
       "      <td>0.295491</td>\n",
       "      <td>0.221220</td>\n",
       "      <td>0.368170</td>\n",
       "      <td>0.670557</td>\n",
       "      <td>0.003183</td>\n",
       "      <td>0.122016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.186524</td>\n",
       "      <td>0.480189</td>\n",
       "      <td>0.397293</td>\n",
       "      <td>0.491586</td>\n",
       "      <td>0.138757</td>\n",
       "      <td>0.469943</td>\n",
       "      <td>0.135028</td>\n",
       "      <td>0.481405</td>\n",
       "      <td>0.301840</td>\n",
       "      <td>0.489700</td>\n",
       "      <td>0.316024</td>\n",
       "      <td>0.388948</td>\n",
       "      <td>0.490875</td>\n",
       "      <td>0.456384</td>\n",
       "      <td>0.415179</td>\n",
       "      <td>0.482436</td>\n",
       "      <td>0.470136</td>\n",
       "      <td>0.056343</td>\n",
       "      <td>0.327391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           alcohol       amphet         amyl       benzos         caff  \\\n",
       "count  1885.000000  1885.000000  1885.000000  1885.000000  1885.000000   \n",
       "mean      0.963926     0.360212     0.196286     0.407958     0.980371   \n",
       "std       0.186524     0.480189     0.397293     0.491586     0.138757   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       1.000000     0.000000     0.000000     0.000000     1.000000   \n",
       "50%       1.000000     0.000000     0.000000     0.000000     1.000000   \n",
       "75%       1.000000     1.000000     0.000000     1.000000     1.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          cannabis         choc         coke        crack      ecstasy  \\\n",
       "count  1885.000000  1885.000000  1885.000000  1885.000000  1885.000000   \n",
       "mean      0.671088     0.981432     0.364456     0.101326     0.398408   \n",
       "std       0.469943     0.135028     0.481405     0.301840     0.489700   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     1.000000     0.000000     0.000000     0.000000   \n",
       "50%       1.000000     1.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     1.000000     1.000000     0.000000     1.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "            heroin     ketamine       legalh          lsd         meth  \\\n",
       "count  1885.000000  1885.000000  1885.000000  1885.000000  1885.000000   \n",
       "mean      0.112467     0.185676     0.404244     0.295491     0.221220   \n",
       "std       0.316024     0.388948     0.490875     0.456384     0.415179   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.000000     1.000000     1.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "         mushrooms     nicotine        semer          vsa  \n",
       "count  1885.000000  1885.000000  1885.000000  1885.000000  \n",
       "mean      0.368170     0.670557     0.003183     0.122016  \n",
       "std       0.482436     0.470136     0.056343     0.327391  \n",
       "min       0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     0.000000     0.000000  \n",
       "50%       0.000000     1.000000     0.000000     0.000000  \n",
       "75%       1.000000     1.000000     0.000000     0.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_mushroom = y['mushrooms']\n",
    "y_choc = y['choc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_mushroom_train, X_mushroom_test, y_mushroom_train, y_mushroom_test = train_test_split(X, y_mushroom, shuffle=True, stratify=y_mushroom, random_state=42, test_size=0.33)\n",
    "X_choc_train, X_choc_test, y_choc_train, y_choc_test = train_test_split(X, y_choc, shuffle=True, stratify=y_choc, random_state=42, test_size=0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mushrooms\n",
       "0    1191\n",
       "1     694\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_mushroom.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "choc\n",
       "1    1850\n",
       "0      35\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_choc.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree, ensemble, svm, neural_network, neighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "\n",
    "def finetune_and_eval(X_train, y_train, X_test, y_test):\n",
    "    clfs = {\n",
    "        'GB': ensemble.GradientBoostingClassifier(),\n",
    "    }\n",
    "\n",
    "    params_space = {\n",
    "        'GB': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'learning_rate': [0.01, 0.1, 0.2],\n",
    "            'max_depth': [3, 5, 10],\n",
    "            'min_samples_split': [2, 10, 20],\n",
    "            'min_samples_leaf': [1, 5, 10],\n",
    "            'subsample': [0.8, 1.0],\n",
    "            'max_features': ['sqrt', 'log2', None]\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Placeholder for storing the best models and parameters\n",
    "    best_estimators = {}\n",
    "    best_params = {}\n",
    "\n",
    "    # Loop through each classifier and perform grid search\n",
    "    for clf_name, clf in clfs.items():\n",
    "        f1 = make_scorer(f1_score , average='micro')\n",
    "        print(f\"Running GridSearchCV for {clf_name}...\")\n",
    "        grid_search = GridSearchCV(estimator=clf, param_grid=params_space[clf_name], cv=5, scoring=f1, n_jobs=-1)\n",
    "        \n",
    "        # Fit grid search on the data\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Store the best estimator and parameters\n",
    "        best_estimators[clf_name] = grid_search.best_estimator_\n",
    "        best_params[clf_name] = grid_search.best_params_\n",
    "        \n",
    "        # Print the best parameters for each classifier\n",
    "        print(f\"Best parameters for {clf_name}: {grid_search.best_params_}\")\n",
    "        print(f\"Best score for {clf_name}: {grid_search.best_score_}\")\n",
    "\n",
    "\n",
    "    # Placeholder for confusion matrices, recalls, and precisions\n",
    "    confusion_matrices = {}\n",
    "    recalls = {}\n",
    "    precisions = {}\n",
    "    f1score = {}\n",
    "    aucValue = {}\n",
    "\n",
    "    # Fit each classifier on the training data and evaluate on the test data\n",
    "    for clf_name, clf in best_estimators.items():\n",
    "        # Fit the classifier on the training data\n",
    "        clf.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict on the test data\n",
    "        y_pred = clf.predict(X_test)\n",
    "        \n",
    "        # Calculate confusion matrix, recall, and precision\n",
    "        confusion_matrices[clf_name] = confusion_matrix(y_test, y_pred)\n",
    "        recalls[clf_name] = recall_score(y_test, y_pred, zero_division=1)  # 'macro' to handle multi-class cases\n",
    "        precisions[clf_name] = precision_score(y_test, y_pred, zero_division=1)\n",
    "        f1score[clf_name] = f1_score(y_test, y_pred)\n",
    "        aucValue[clf_name] = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"confusion matrix: {confusion_matrices['GB']}\")\n",
    "    print(f\"recall: {recalls['GB']}\")\n",
    "    print(f\"precision: {precisions['GB']}\")\n",
    "    print(f\"f1: {f1score['GB']}\")\n",
    "    print(f\"auc: {aucValue['GB']}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(y_test, y_pred):\n",
    "    recall = recall_score(y_test, y_pred, zero_division=1)  # 'macro' to handle multi-class cases\n",
    "    precision = precision_score(y_test, y_pred, zero_division=1)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "    return {\n",
    "        'recall': recall,\n",
    "        'precision': precision,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running GridSearchCV for GB...\n",
      "Best parameters for GB: {'learning_rate': 0.2, 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 20, 'n_estimators': 50, 'subsample': 0.8}\n",
      "Best score for GB: 0.9825710521362694\n",
      "confusion matrix: [[  0  12]\n",
      " [  0 611]]\n",
      "recall: 1.0\n",
      "precision: 0.9807383627608347\n",
      "f1: 0.9902755267423015\n",
      "auc: 0.5\n"
     ]
    }
   ],
   "source": [
    "finetune_and_eval(X_choc_train, y_choc_train, X_choc_test, y_choc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "import numpy as np\n",
    "\n",
    "class CoTrainingClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, estimator1, estimator2, n_iter=10, p=1, n=1, \n",
    "                 view1_features=None, view2_features=None):\n",
    "        self.estimator1 = clone(estimator1)\n",
    "        self.estimator2 = clone(estimator2)\n",
    "        self.n_iter = n_iter\n",
    "        self.p = p  # Number of positive samples to add each iteration\n",
    "        self.n = n  # Number of negative samples to add each iteration\n",
    "        self.view1_features = view1_features\n",
    "        self.view2_features = view2_features\n",
    "\n",
    "    def fit(self, X_labeled, y_labeled, X_unlabeled):\n",
    "        # Initialize labeled data for each estimator\n",
    "        L1_X = X_labeled.copy()\n",
    "        L1_y = y_labeled.copy()\n",
    "        L2_X = X_labeled.copy()\n",
    "        L2_y = y_labeled.copy()\n",
    "        U_X = X_unlabeled.copy()\n",
    "\n",
    "        # Initialize feature views\n",
    "        if self.view1_features is not None:\n",
    "            L1_X_view = L1_X[:, self.view1_features]\n",
    "        else:\n",
    "            L1_X_view = L1_X\n",
    "\n",
    "        if self.view2_features is not None:\n",
    "            L2_X_view = L2_X[:, self.view2_features]\n",
    "        else:\n",
    "            L2_X_view = L2_X\n",
    "\n",
    "        U_index = np.arange(len(U_X))\n",
    "\n",
    "        for i in range(self.n_iter):\n",
    "            # Extract features for estimator1\n",
    "            if self.view1_features is not None:\n",
    "                U_X_view1 = U_X[:, self.view1_features]\n",
    "            else:\n",
    "                U_X_view1 = U_X\n",
    "            \n",
    "            # Train estimator1\n",
    "            self.estimator1.fit(L1_X_view, L1_y)\n",
    "            # Predict on U\n",
    "            probs1 = self.estimator1.predict_proba(U_X_view1)\n",
    "            # Find most confident positive and negative predictions\n",
    "            pos_confidence1 = probs1[:, 1]\n",
    "            neg_confidence1 = probs1[:, 0]\n",
    "            pos_indices = np.argsort(-pos_confidence1)\n",
    "            neg_indices = np.argsort(-neg_confidence1)\n",
    "            # Select top p positive and n negative samples\n",
    "            pos_samples1 = min(self.p, len(pos_indices))\n",
    "            neg_samples1 = min(self.n, len(neg_indices))\n",
    "            pos_idx1 = pos_indices[:pos_samples1]\n",
    "            neg_idx1 = neg_indices[:neg_samples1]\n",
    "            idx1 = np.concatenate([pos_idx1, neg_idx1])\n",
    "            # Get corresponding samples and labels\n",
    "            new_samples1 = U_X[idx1]\n",
    "            new_labels1 = np.array([1]*pos_samples1 + [0]*neg_samples1)\n",
    "            # Remove selected samples from U_X\n",
    "            mask = np.ones(len(U_X), dtype=bool)\n",
    "            mask[idx1] = False\n",
    "            U_X = U_X[mask]\n",
    "            U_index = U_index[mask]\n",
    "\n",
    "            # Add new samples to L2\n",
    "            L2_X = np.vstack([L2_X, new_samples1])\n",
    "            if self.view2_features is not None:\n",
    "                new_samples1_view = new_samples1[:, self.view2_features]\n",
    "                L2_X_view = np.vstack([L2_X_view, new_samples1_view])\n",
    "            else:\n",
    "                L2_X_view = np.vstack([L2_X_view, new_samples1])\n",
    "            L2_y = np.concatenate([L2_y, new_labels1])\n",
    "\n",
    "            # If no more samples to label, break\n",
    "            if len(U_X) == 0:\n",
    "                break\n",
    "\n",
    "            # Extract features for estimator2\n",
    "            if self.view2_features is not None:\n",
    "                U_X_view2 = U_X[:, self.view2_features]\n",
    "            else:\n",
    "                U_X_view2 = U_X\n",
    "\n",
    "            # Train estimator2\n",
    "            self.estimator2.fit(L2_X_view, L2_y)\n",
    "            # Predict on U\n",
    "            probs2 = self.estimator2.predict_proba(U_X_view2)\n",
    "            # Find most confident positive and negative predictions\n",
    "            pos_confidence2 = probs2[:, 1]\n",
    "            neg_confidence2 = probs2[:, 0]\n",
    "            pos_indices2 = np.argsort(-pos_confidence2)\n",
    "            neg_indices2 = np.argsort(-neg_confidence2)\n",
    "            pos_samples2 = min(self.p, len(pos_indices2))\n",
    "            neg_samples2 = min(self.n, len(neg_indices2))\n",
    "            pos_idx2 = pos_indices2[:pos_samples2]\n",
    "            neg_idx2 = neg_indices2[:neg_samples2]\n",
    "            idx2 = np.concatenate([pos_idx2, neg_idx2])\n",
    "            new_samples2 = U_X[idx2]\n",
    "            new_labels2 = np.array([1]*pos_samples2 + [0]*neg_samples2)\n",
    "            # Remove selected samples from U_X\n",
    "            mask = np.ones(len(U_X), dtype=bool)\n",
    "            mask[idx2] = False\n",
    "            U_X = U_X[mask]\n",
    "            U_index = U_index[mask]\n",
    "\n",
    "            # Add new samples to L1\n",
    "            L1_X = np.vstack([L1_X, new_samples2])\n",
    "            if self.view1_features is not None:\n",
    "                new_samples2_view = new_samples2[:, self.view1_features]\n",
    "                L1_X_view = np.vstack([L1_X_view, new_samples2_view])\n",
    "            else:\n",
    "                L1_X_view = np.vstack([L1_X_view, new_samples2])\n",
    "            L1_y = np.concatenate([L1_y, new_labels2])\n",
    "\n",
    "            # If no more samples to label, break\n",
    "            if len(U_X) == 0:\n",
    "                break\n",
    "\n",
    "        # Final training\n",
    "        self.estimator1.fit(L1_X_view, L1_y)\n",
    "        self.estimator2.fit(L2_X_view, L2_y)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.view1_features is not None:\n",
    "            X_view1 = X[:, self.view1_features]\n",
    "        else:\n",
    "            X_view1 = X\n",
    "        if self.view2_features is not None:\n",
    "            X_view2 = X[:, self.view2_features]\n",
    "        else:\n",
    "            X_view2 = X\n",
    "\n",
    "        # Predict with both estimators and combine predictions\n",
    "        pred1 = self.estimator1.predict_proba(X_view1)\n",
    "        pred2 = self.estimator2.predict_proba(X_view2)\n",
    "        avg_pred = (pred1 + pred2) / 2\n",
    "        return np.argmax(avg_pred, axis=1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if self.view1_features is not None:\n",
    "            X_view1 = X[:, self.view1_features]\n",
    "        else:\n",
    "            X_view1 = X\n",
    "        if self.view2_features is not None:\n",
    "            X_view2 = X[:, self.view2_features]\n",
    "        else:\n",
    "            X_view2 = X\n",
    "\n",
    "        pred1 = self.estimator1.predict_proba(X_view1)\n",
    "        pred2 = self.estimator2.predict_proba(X_view2)\n",
    "        avg_pred = (pred1 + pred2) / 2\n",
    "        return avg_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, \n",
    "                           n_informative=15, n_redundant=5, \n",
    "                           random_state=42)\n",
    "\n",
    "# Split into labeled and unlabeled data\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Assume we have only 50 labeled samples\n",
    "n_labeled = 50\n",
    "X_labeled = X_train_full[:n_labeled]\n",
    "y_labeled = y_train_full[:n_labeled]\n",
    "X_unlabeled = X_train_full[n_labeled:]\n",
    "\n",
    "# Define two different estimators\n",
    "estimator1 = LogisticRegression()\n",
    "estimator2 = DecisionTreeClassifier()\n",
    "\n",
    "# Define feature subsets for each estimator (views)\n",
    "view1 = np.arange(10)  # First 10 features\n",
    "view2 = np.arange(10, 20)  # Last 10 features\n",
    "\n",
    "# Initialize Co-Training classifier\n",
    "co_clf = CoTrainingClassifier(estimator1, estimator2, \n",
    "                              n_iter=10, p=5, n=5, \n",
    "                              view1_features=view1, \n",
    "                              view2_features=view2)\n",
    "\n",
    "# Fit the model\n",
    "co_clf.fit(X_labeled, y_labeled, X_unlabeled)\n",
    "\n",
    "# Evaluate on the test set\n",
    "y_pred = co_clf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin, clone\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.utils.validation import check_X_y, check_array\n",
    "import numpy as np\n",
    "\n",
    "class SemiSupervisedEnsembleClassifier(BaseEstimator, ClassifierMixin):\n",
    "    \"\"\"\n",
    "    An ensemble of semi-supervised classifiers that fits within the sklearn interface.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_estimator=None, n_estimators=10, voting='hard'):\n",
    "        \"\"\"\n",
    "        Initializes the ensemble classifier.\n",
    "\n",
    "        Parameters:\n",
    "        - base_estimator: The base estimator to use for each ensemble member.\n",
    "        - n_estimators: The number of estimators in the ensemble.\n",
    "        - voting: 'hard' for majority voting, 'soft' for averaging predicted probabilities.\n",
    "        \"\"\"\n",
    "        self.base_estimator = base_estimator\n",
    "        self.n_estimators = n_estimators\n",
    "        self.voting = voting  # 'hard' or 'soft'\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fits the ensemble of classifiers on the provided data.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like of shape (n_samples, n_features), the training input samples.\n",
    "        - y: array-like of shape (n_samples,), the target values with unlabeled samples marked as -1.\n",
    "\n",
    "        Returns:\n",
    "        - self: Fitted estimator.\n",
    "        \"\"\"\n",
    "        # Validate the input data\n",
    "        X, y = check_X_y(X, y, accept_sparse=True)\n",
    "        self.classes_ = np.unique(y[y != -1])\n",
    "        self.estimators_ = []\n",
    "\n",
    "        # Create and fit each estimator in the ensemble\n",
    "        for _ in range(self.n_estimators):\n",
    "            estimator = clone(self.base_estimator)\n",
    "            self_training_estimator = SelfTrainingClassifier(estimator)\n",
    "            self_training_estimator.fit(X, y)\n",
    "            self.estimators_.append(self_training_estimator)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class labels for samples in X.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like of shape (n_samples, n_features), the input samples.\n",
    "\n",
    "        Returns:\n",
    "        - y_pred: array-like of shape (n_samples,), the predicted classes.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse=True)\n",
    "\n",
    "        if self.voting == 'hard':\n",
    "            # Collect predictions from each estimator\n",
    "            predictions = np.asarray([estimator.predict(X) for estimator in self.estimators_]).T\n",
    "            # Majority vote\n",
    "            y_pred = np.apply_along_axis(\n",
    "                lambda x: np.bincount(x, minlength=len(self.classes_)).argmax(), axis=1, arr=predictions\n",
    "            )\n",
    "            return y_pred\n",
    "        elif self.voting == 'soft':\n",
    "            # Average predicted probabilities\n",
    "            probas = np.asarray([estimator.predict_proba(X) for estimator in self.estimators_])\n",
    "            avg_proba = np.mean(probas, axis=0)\n",
    "            y_pred = self.classes_[np.argmax(avg_proba, axis=1)]\n",
    "            return y_pred\n",
    "        else:\n",
    "            raise ValueError(\"Voting must be 'hard' or 'soft'\")\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predicts class probabilities for samples in X.\n",
    "\n",
    "        Parameters:\n",
    "        - X: array-like of shape (n_samples, n_features), the input samples.\n",
    "\n",
    "        Returns:\n",
    "        - avg_proba: array-like of shape (n_samples, n_classes), the class probabilities.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse=True)\n",
    "        probas = np.asarray([estimator.predict_proba(X) for estimator in self.estimators_])\n",
    "        avg_proba = np.mean(probas, axis=0)\n",
    "        return avg_proba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8269230769230769\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "\n",
    "# Introduce unlabeled data (-1)\n",
    "rng = np.random.RandomState(42)\n",
    "unlabeled_mask = rng.rand(len(y)) < 0.7  # 70% unlabeled\n",
    "y_unlabeled = y.copy()\n",
    "y_unlabeled[unlabeled_mask] = -1\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_unlabeled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the ensemble classifier with a Decision Tree as the base estimator\n",
    "ensemble_classifier = SemiSupervisedEnsembleClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    n_estimators=5,\n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "ensemble_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = ensemble_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_test[y_test != -1], y_pred[y_test != -1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "class AutoencoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, hidden_layer_sizes=(32,), activation='relu', solver='adam', \n",
    "                 batch_size='auto', learning_rate_init=0.001, max_iter=200, random_state=None):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self._is_fitted = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        n_features = X.shape[1]\n",
    "        # Create a symmetric autoencoder architecture\n",
    "        hidden_sizes = list(self.hidden_layer_sizes)\n",
    "        # Encoder\n",
    "        encoder_layer_sizes = hidden_sizes\n",
    "        # Decoder (reverse of encoder)\n",
    "        decoder_layer_sizes = hidden_sizes[::-1]\n",
    "        # Total layer sizes\n",
    "        layer_sizes = encoder_layer_sizes + decoder_layer_sizes\n",
    "        # Initialize the MLPRegressor\n",
    "        self.autoencoder = MLPRegressor(\n",
    "            hidden_layer_sizes=layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver=self.solver,\n",
    "            batch_size=self.batch_size,\n",
    "            learning_rate_init=self.learning_rate_init,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        # Fit the autoencoder to reconstruct the input\n",
    "        self.autoencoder.fit(X, X)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, '_is_fitted')\n",
    "        # Compute the activations of each layer\n",
    "        X_transformed = self._compute_hidden_activations(X)\n",
    "        # Return the output of the last encoder layer (the bottleneck layer)\n",
    "        # Assuming the encoder layers are the first half\n",
    "        n_encoder_layers = len(self.hidden_layer_sizes)\n",
    "        return X_transformed[n_encoder_layers - 1]\n",
    "\n",
    "    def _compute_hidden_activations(self, X):\n",
    "        # This method computes the activations at each hidden layer\n",
    "        activations = [X]\n",
    "        for i in range(len(self.autoencoder.coefs_)):\n",
    "            activation = np.dot(activations[i], self.autoencoder.coefs_[i]) + self.autoencoder.intercepts_[i]\n",
    "            if i < len(self.autoencoder.coefs_) - 1:\n",
    "                activation = self._activation_function(activation)\n",
    "            activations.append(activation)\n",
    "        return activations[1:-1]  # Exclude input and output layers\n",
    "\n",
    "    def _activation_function(self, X):\n",
    "        # Apply the activation function\n",
    "        if self.activation == 'identity':\n",
    "            return X\n",
    "        elif self.activation == 'logistic':\n",
    "            return 1 / (1 + np.exp(-X))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(X)\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, X)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function '{self.activation}'\")\n",
    "\n",
    "class SemiSupervisedAutoencoderClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_classifier=None, hidden_layer_sizes=(32,), activation='relu', \n",
    "                 solver='adam', batch_size='auto', learning_rate_init=0.001, max_iter=200, \n",
    "                 random_state=None):\n",
    "        self.base_classifier = base_classifier or SVC(probability=True, random_state=random_state)\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self._is_fitted = False\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.asarray(y)\n",
    "        # Identify unlabeled data (label == -1 or None)\n",
    "        unlabeled_mask = (y == -1) | (y == None)\n",
    "        X_labeled = X[~unlabeled_mask]\n",
    "        y_labeled = y[~unlabeled_mask]\n",
    "        X_unlabeled = X[unlabeled_mask]\n",
    "        # Combine all data for autoencoder training\n",
    "        X_all = np.vstack([X_labeled, X_unlabeled])\n",
    "        # Fit the autoencoder transformer\n",
    "        self.autoencoder = AutoencoderTransformer(\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver=self.solver,\n",
    "            batch_size=self.batch_size,\n",
    "            learning_rate_init=self.learning_rate_init,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        self.autoencoder.fit(X_all)\n",
    "        # Transform data using the trained autoencoder\n",
    "        X_labeled_encoded = self.autoencoder.transform(X_labeled)\n",
    "        X_unlabeled_encoded = self.autoencoder.transform(X_unlabeled)\n",
    "        X_encoded = np.vstack([X_labeled_encoded, X_unlabeled_encoded])\n",
    "        # Prepare labels for semi-supervised learning\n",
    "        y_full = np.concatenate([y_labeled, np.full(X_unlabeled.shape[0], -1)])\n",
    "        # Train the semi-supervised classifier\n",
    "        self.classifier = SelfTrainingClassifier(self.base_classifier)\n",
    "        self.classifier.fit(X_encoded, y_full)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, '_is_fitted')\n",
    "        X_encoded = self.autoencoder.transform(X)\n",
    "        return self.classifier.predict(X_encoded)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self, '_is_fitted')\n",
    "        X_encoded = self.autoencoder.transform(X)\n",
    "        return self.classifier.predict_proba(X_encoded)\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin, TransformerMixin\n",
    "from sklearn.semi_supervised import SelfTrainingClassifier\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "class AutoencoderTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, hidden_layer_sizes=(32,), activation='relu', solver='adam', \n",
    "                 batch_size='auto', learning_rate_init=0.001, max_iter=200, random_state=None):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self._is_fitted = False\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        n_features = X.shape[1]\n",
    "        # Create a symmetric autoencoder architecture\n",
    "        hidden_sizes = list(self.hidden_layer_sizes)\n",
    "        # Encoder\n",
    "        encoder_layer_sizes = hidden_sizes\n",
    "        # Decoder (reverse of encoder)\n",
    "        decoder_layer_sizes = hidden_sizes[::-1]\n",
    "        # Total layer sizes\n",
    "        layer_sizes = encoder_layer_sizes + decoder_layer_sizes\n",
    "        # Initialize the MLPRegressor\n",
    "        self.autoencoder = MLPRegressor(\n",
    "            hidden_layer_sizes=layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver=self.solver,\n",
    "            batch_size=self.batch_size,\n",
    "            learning_rate_init=self.learning_rate_init,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        # Fit the autoencoder to reconstruct the input\n",
    "        self.autoencoder.fit(X, X)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        check_is_fitted(self, '_is_fitted')\n",
    "        # Compute the activations of each layer\n",
    "        X_transformed = self._compute_hidden_activations(X)\n",
    "        # Return the output of the last encoder layer (the bottleneck layer)\n",
    "        # Assuming the encoder layers are the first half\n",
    "        n_encoder_layers = len(self.hidden_layer_sizes)\n",
    "        return X_transformed[n_encoder_layers - 1]\n",
    "\n",
    "    def _compute_hidden_activations(self, X):\n",
    "        # This method computes the activations at each hidden layer\n",
    "        activations = [X]\n",
    "        for i in range(len(self.autoencoder.coefs_)):\n",
    "            activation = np.dot(activations[i], self.autoencoder.coefs_[i]) + self.autoencoder.intercepts_[i]\n",
    "            if i < len(self.autoencoder.coefs_) - 1:\n",
    "                activation = self._activation_function(activation)\n",
    "            activations.append(activation)\n",
    "        return activations[1:-1]  # Exclude input and output layers\n",
    "\n",
    "    def _activation_function(self, X):\n",
    "        # Apply the activation function\n",
    "        if self.activation == 'identity':\n",
    "            return X\n",
    "        elif self.activation == 'logistic':\n",
    "            return 1 / (1 + np.exp(-X))\n",
    "        elif self.activation == 'tanh':\n",
    "            return np.tanh(X)\n",
    "        elif self.activation == 'relu':\n",
    "            return np.maximum(0, X)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function '{self.activation}'\")\n",
    "\n",
    "class SemiSupervisedAutoencoderClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, base_classifier=None, hidden_layer_sizes=(32,), activation='relu', \n",
    "                 solver='adam', batch_size='auto', learning_rate_init=0.001, max_iter=200, \n",
    "                 random_state=None):\n",
    "        self.base_classifier = base_classifier or SVC(probability=True, random_state=random_state)\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate_init = learning_rate_init\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self._is_fitted = False\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        y = np.asarray(y)\n",
    "        # Identify unlabeled data (label == -1 or None)\n",
    "        unlabeled_mask = (y == -1) | (y == None)\n",
    "        X_labeled = X[~unlabeled_mask]\n",
    "        y_labeled = y[~unlabeled_mask]\n",
    "        X_unlabeled = X[unlabeled_mask]\n",
    "        # Combine all data for autoencoder training\n",
    "        X_all = np.vstack([X_labeled, X_unlabeled])\n",
    "        # Fit the autoencoder transformer\n",
    "        self.autoencoder = AutoencoderTransformer(\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver=self.solver,\n",
    "            batch_size=self.batch_size,\n",
    "            learning_rate_init=self.learning_rate_init,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state\n",
    "        )\n",
    "        self.autoencoder.fit(X_all)\n",
    "        # Transform data using the trained autoencoder\n",
    "        X_labeled_encoded = self.autoencoder.transform(X_labeled)\n",
    "        X_unlabeled_encoded = self.autoencoder.transform(X_unlabeled)\n",
    "        X_encoded = np.vstack([X_labeled_encoded, X_unlabeled_encoded])\n",
    "        # Prepare labels for semi-supervised learning\n",
    "        y_full = np.concatenate([y_labeled, np.full(X_unlabeled.shape[0], -1)])\n",
    "        # Train the semi-supervised classifier\n",
    "        self.classifier = SelfTrainingClassifier(self.base_classifier)\n",
    "        self.classifier.fit(X_encoded, y_full)\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, '_is_fitted')\n",
    "        X_encoded = self.autoencoder.transform(X)\n",
    "        return self.classifier.predict(X_encoded)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        check_is_fitted(self, '_is_fitted')\n",
    "        X_encoded = self.autoencoder.transform(X)\n",
    "        return self.classifier.predict_proba(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 20)\n",
      "(200, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Generate synthetic data\n",
    "X_full, y_full = make_classification(n_samples=1000, n_features=20, n_classes=2, random_state=42)\n",
    "# Simulate unlabeled data by setting some labels to -1\n",
    "y_semi = y_full.copy()\n",
    "y_semi[500:] = -1  # Assume the last 500 samples are unlabeled\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_full, y_semi, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize the semi-supervised classifier\n",
    "model = SemiSupervisedAutoencoderClassifier(\n",
    "    base_classifier=SVC(probability=True, random_state=42),\n",
    "    hidden_layer_sizes=(10,),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=200,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test[y_test != -1], y_pred[y_test != -1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiemnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# input must be numpy ndarray nor pd.DataFrame\n",
    "def semi_experiement(X_train, X_test, y_train, y_test):\n",
    "    unlabelled_ratio = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "\n",
    "    experiments_results = {}\n",
    "\n",
    "    for ratio in unlabelled_ratio:\n",
    "        ratio_result = {}\n",
    "\n",
    "        X_train_labeled, X_train_unlabeled, y_train_labeled, y_train_unlabeled = train_test_split(X_train, y_train, stratify=y_train, test_size=ratio, random_state=42)\n",
    "        y_train_unlabeled[:] = -1 # for sklearn SelfTrainingClassifer\n",
    "\n",
    "        print(f'semi classifiers training, unlabeled ratio: {ratio}')\n",
    "\n",
    "        X_all = np.concatenate([X_train_labeled, X_train_unlabeled])\n",
    "        y_all = np.concatenate([y_train_labeled, y_train_unlabeled])\n",
    "        # base classifier\n",
    "        base_classifier = LogisticRegression(max_iter=1000, random_state=42)\n",
    "        self_training_model = SelfTrainingClassifier(base_classifier)\n",
    "        self_training_model.fit(X_all, y_all)\n",
    "        y_pred = self_training_model.predict(X_test)\n",
    "        evaluation = eval_model(y_test, y_pred)\n",
    "        ratio_result['self'] = evaluation\n",
    "\n",
    "        # cotrain\n",
    "        estimator1 = LogisticRegression()\n",
    "        estimator2 = DecisionTreeClassifier()\n",
    "        view1 = np.arange(6) # first 6 features\n",
    "        view2 = np.arange(6, 12) # last 6 features\n",
    "\n",
    "        co_clf = CoTrainingClassifier(estimator1, estimator2, \n",
    "                                n_iter=10, p=5, n=5, \n",
    "                                view1_features=view1, \n",
    "                                view2_features=view2)\n",
    "\n",
    "        co_clf.fit(X_train_labeled, y_train_labeled, X_train_unlabeled)\n",
    "        y_pred = co_clf.predict(X_test)\n",
    "        evaluation = eval_model(y_test, y_pred)\n",
    "        ratio_result['cotrain'] = evaluation\n",
    "\n",
    "        # semi-supervised ensemble\n",
    "        ensemble_clf = SemiSupervisedEnsembleClassifier(\n",
    "            base_estimator=DecisionTreeClassifier(),\n",
    "            n_estimators=5,\n",
    "            voting='hard'\n",
    "        )\n",
    "\n",
    "        ensemble_clf.fit(X_all, y_all)\n",
    "        y_pred = ensemble_clf.predict(X_test)\n",
    "        evaluation = eval_model(y_test, y_pred)\n",
    "        ratio_result['ensemble'] = evaluation\n",
    "\n",
    "        # pretrained/autoencoder\n",
    "        pretrained_clf = SemiSupervisedAutoencoderClassifier(\n",
    "            base_classifier=SVC(probability=True, random_state=42),\n",
    "            hidden_layer_sizes=(5,),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            learning_rate_init=0.001,\n",
    "            max_iter=200,\n",
    "            random_state=42\n",
    "        )\n",
    "        pretrained_clf.fit(X_all, y_all)\n",
    "        y_pred = pretrained_clf.predict(X_test)\n",
    "        ratio_result['pretrained'] = evaluation\n",
    "\n",
    "        experiments_results[ratio] = ratio_result\n",
    "\n",
    "        print(ratio_result)\n",
    "    \n",
    "    return experiments_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semi classifiers training, unlabeled ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': {'recall': 1.0, 'precision': 0.9807383627608347, 'f1': 0.9902755267423015, 'auc': 0.5}, 'cotrain': {'recall': 0.9476268412438625, 'precision': 0.9813559322033898, 'f1': 0.9641965029142381, 'auc': 0.515480087288598}, 'ensemble': {'recall': 0.9852700490998363, 'precision': 0.9820554649265906, 'f1': 0.9836601307189542, 'auc': 0.5343016912165849}, 'pretrained': {'recall': 0.9852700490998363, 'precision': 0.9820554649265906, 'f1': 0.9836601307189542, 'auc': 0.5343016912165849}}\n",
      "semi classifiers training, unlabeled ratio: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': {'recall': 1.0, 'precision': 0.9807383627608347, 'f1': 0.9902755267423015, 'auc': 0.5}, 'cotrain': {'recall': 0.9198036006546645, 'precision': 0.9808027923211169, 'f1': 0.9493243243243243, 'auc': 0.5015684669939989}, 'ensemble': {'recall': 0.9869067103109657, 'precision': 0.9804878048780488, 'f1': 0.9836867862969005, 'auc': 0.49345335515548283}, 'pretrained': {'recall': 0.9869067103109657, 'precision': 0.9804878048780488, 'f1': 0.9836867862969005, 'auc': 0.49345335515548283}}\n",
      "semi classifiers training, unlabeled ratio: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': {'recall': 1.0, 'precision': 0.9807383627608347, 'f1': 0.9902755267423015, 'auc': 0.5}, 'cotrain': {'recall': 0.9296235679214403, 'precision': 0.9810017271157168, 'f1': 0.9546218487394958, 'auc': 0.5064784506273868}, 'ensemble': {'recall': 0.9754500818330606, 'precision': 0.9818780889621087, 'f1': 0.9786535303776683, 'auc': 0.529391707583197}, 'pretrained': {'recall': 0.9754500818330606, 'precision': 0.9818780889621087, 'f1': 0.9786535303776683, 'auc': 0.529391707583197}}\n",
      "semi classifiers training, unlabeled ratio: 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': {'recall': 1.0, 'precision': 0.9807383627608347, 'f1': 0.9902755267423015, 'auc': 0.5}, 'cotrain': {'recall': 0.8936170212765957, 'precision': 0.9802513464991023, 'f1': 0.934931506849315, 'auc': 0.48847517730496454}, 'ensemble': {'recall': 0.9803600654664485, 'precision': 0.9803600654664485, 'f1': 0.9803600654664485, 'auc': 0.49018003273322425}, 'pretrained': {'recall': 0.9803600654664485, 'precision': 0.9803600654664485, 'f1': 0.9803600654664485, 'auc': 0.49018003273322425}}\n",
      "semi classifiers training, unlabeled ratio: 0.5\n",
      "{'self': {'recall': 1.0, 'precision': 0.9807383627608347, 'f1': 0.9902755267423015, 'auc': 0.5}, 'cotrain': {'recall': 0.8821603927986906, 'precision': 0.98, 'f1': 0.9285099052540913, 'auc': 0.482746863066012}, 'ensemble': {'recall': 0.9819967266775778, 'precision': 0.9803921568627451, 'f1': 0.9811937857726901, 'auc': 0.4909983633387889}, 'pretrained': {'recall': 0.9819967266775778, 'precision': 0.9803921568627451, 'f1': 0.9811937857726901, 'auc': 0.4909983633387889}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.1: {'self': {'recall': 1.0,\n",
       "   'precision': 0.9807383627608347,\n",
       "   'f1': 0.9902755267423015,\n",
       "   'auc': 0.5},\n",
       "  'cotrain': {'recall': 0.9476268412438625,\n",
       "   'precision': 0.9813559322033898,\n",
       "   'f1': 0.9641965029142381,\n",
       "   'auc': 0.515480087288598},\n",
       "  'ensemble': {'recall': 0.9852700490998363,\n",
       "   'precision': 0.9820554649265906,\n",
       "   'f1': 0.9836601307189542,\n",
       "   'auc': 0.5343016912165849},\n",
       "  'pretrained': {'recall': 0.9852700490998363,\n",
       "   'precision': 0.9820554649265906,\n",
       "   'f1': 0.9836601307189542,\n",
       "   'auc': 0.5343016912165849}},\n",
       " 0.2: {'self': {'recall': 1.0,\n",
       "   'precision': 0.9807383627608347,\n",
       "   'f1': 0.9902755267423015,\n",
       "   'auc': 0.5},\n",
       "  'cotrain': {'recall': 0.9198036006546645,\n",
       "   'precision': 0.9808027923211169,\n",
       "   'f1': 0.9493243243243243,\n",
       "   'auc': 0.5015684669939989},\n",
       "  'ensemble': {'recall': 0.9869067103109657,\n",
       "   'precision': 0.9804878048780488,\n",
       "   'f1': 0.9836867862969005,\n",
       "   'auc': 0.49345335515548283},\n",
       "  'pretrained': {'recall': 0.9869067103109657,\n",
       "   'precision': 0.9804878048780488,\n",
       "   'f1': 0.9836867862969005,\n",
       "   'auc': 0.49345335515548283}},\n",
       " 0.3: {'self': {'recall': 1.0,\n",
       "   'precision': 0.9807383627608347,\n",
       "   'f1': 0.9902755267423015,\n",
       "   'auc': 0.5},\n",
       "  'cotrain': {'recall': 0.9296235679214403,\n",
       "   'precision': 0.9810017271157168,\n",
       "   'f1': 0.9546218487394958,\n",
       "   'auc': 0.5064784506273868},\n",
       "  'ensemble': {'recall': 0.9754500818330606,\n",
       "   'precision': 0.9818780889621087,\n",
       "   'f1': 0.9786535303776683,\n",
       "   'auc': 0.529391707583197},\n",
       "  'pretrained': {'recall': 0.9754500818330606,\n",
       "   'precision': 0.9818780889621087,\n",
       "   'f1': 0.9786535303776683,\n",
       "   'auc': 0.529391707583197}},\n",
       " 0.4: {'self': {'recall': 1.0,\n",
       "   'precision': 0.9807383627608347,\n",
       "   'f1': 0.9902755267423015,\n",
       "   'auc': 0.5},\n",
       "  'cotrain': {'recall': 0.8936170212765957,\n",
       "   'precision': 0.9802513464991023,\n",
       "   'f1': 0.934931506849315,\n",
       "   'auc': 0.48847517730496454},\n",
       "  'ensemble': {'recall': 0.9803600654664485,\n",
       "   'precision': 0.9803600654664485,\n",
       "   'f1': 0.9803600654664485,\n",
       "   'auc': 0.49018003273322425},\n",
       "  'pretrained': {'recall': 0.9803600654664485,\n",
       "   'precision': 0.9803600654664485,\n",
       "   'f1': 0.9803600654664485,\n",
       "   'auc': 0.49018003273322425}},\n",
       " 0.5: {'self': {'recall': 1.0,\n",
       "   'precision': 0.9807383627608347,\n",
       "   'f1': 0.9902755267423015,\n",
       "   'auc': 0.5},\n",
       "  'cotrain': {'recall': 0.8821603927986906,\n",
       "   'precision': 0.98,\n",
       "   'f1': 0.9285099052540913,\n",
       "   'auc': 0.482746863066012},\n",
       "  'ensemble': {'recall': 0.9819967266775778,\n",
       "   'precision': 0.9803921568627451,\n",
       "   'f1': 0.9811937857726901,\n",
       "   'auc': 0.4909983633387889},\n",
       "  'pretrained': {'recall': 0.9819967266775778,\n",
       "   'precision': 0.9803921568627451,\n",
       "   'f1': 0.9811937857726901,\n",
       "   'auc': 0.4909983633387889}}}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_choc_train.to_numpy()\n",
    "y_train = y_choc_train.to_numpy()\n",
    "X_test = X_choc_test.to_numpy()\n",
    "y_test = y_choc_test.to_numpy()\n",
    "\n",
    "semi_experiement(X_train, X_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "semi classifiers training, unlabeled ratio: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': {'recall': 0.6200873362445415, 'precision': 0.6761904761904762, 'f1': 0.6469248291571754, 'auc': 0.7237492518786159}, 'cotrain': {'recall': 0.5720524017467249, 'precision': 0.4763636363636364, 'f1': 0.5198412698412699, 'auc': 0.6032850841220935}, 'ensemble': {'recall': 0.5458515283842795, 'precision': 0.5555555555555556, 'f1': 0.5506607929515418, 'auc': 0.6460222108926474}, 'pretrained': {'recall': 0.5458515283842795, 'precision': 0.5555555555555556, 'f1': 0.5506607929515418, 'auc': 0.6460222108926474}}\n",
      "semi classifiers training, unlabeled ratio: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': {'recall': 0.6244541484716157, 'precision': 0.6777251184834123, 'f1': 0.65, 'auc': 0.725932657992153}, 'cotrain': {'recall': 0.5589519650655022, 'precision': 0.4980544747081712, 'f1': 0.5267489711934157, 'auc': 0.6157703987764059}, 'ensemble': {'recall': 0.5895196506550219, 'precision': 0.5895196506550219, 'f1': 0.5895196506550219, 'auc': 0.6754704852259882}, 'pretrained': {'recall': 0.5895196506550219, 'precision': 0.5895196506550219, 'f1': 0.5895196506550219, 'auc': 0.6754704852259882}}\n",
      "semi classifiers training, unlabeled ratio: 0.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': {'recall': 0.6331877729257642, 'precision': 0.6712962962962963, 'f1': 0.651685393258427, 'auc': 0.7264923636202425}, 'cotrain': {'recall': 0.5283842794759825, 'precision': 0.4782608695652174, 'f1': 0.5020746887966805, 'auc': 0.5966794493826613}, 'ensemble': {'recall': 0.6069868995633187, 'precision': 0.5840336134453782, 'f1': 0.5952890792291221, 'auc': 0.6778589320151619}, 'pretrained': {'recall': 0.6069868995633187, 'precision': 0.5840336134453782, 'f1': 0.5952890792291221, 'auc': 0.6778589320151619}}\n",
      "semi classifiers training, unlabeled ratio: 0.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': {'recall': 0.62882096069869, 'precision': 0.6792452830188679, 'f1': 0.6530612244897959, 'auc': 0.7281160641056903}, 'cotrain': {'recall': 0.4890829694323144, 'precision': 0.4890829694323144, 'f1': 0.4890829694323144, 'auc': 0.5960643273557511}, 'ensemble': {'recall': 0.5676855895196506, 'precision': 0.6018518518518519, 'f1': 0.5842696629213483, 'auc': 0.6747057389222619}, 'pretrained': {'recall': 0.5676855895196506, 'precision': 0.6018518518518519, 'f1': 0.5842696629213483, 'auc': 0.6747057389222619}}\n",
      "semi classifiers training, unlabeled ratio: 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chan/miniconda3/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'self': {'recall': 0.6506550218340611, 'precision': 0.680365296803653, 'f1': 0.6651785714285714, 'auc': 0.736495023607386}, 'cotrain': {'recall': 0.5545851528384279, 'precision': 0.516260162601626, 'f1': 0.5347368421052632, 'auc': 0.6262773479928181}, 'ensemble': {'recall': 0.5633187772925764, 'precision': 0.589041095890411, 'f1': 0.5758928571428571, 'auc': 0.6674461906767449}, 'pretrained': {'recall': 0.5633187772925764, 'precision': 0.589041095890411, 'f1': 0.5758928571428571, 'auc': 0.6674461906767449}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0.1: {'self': {'recall': 0.6200873362445415,\n",
       "   'precision': 0.6761904761904762,\n",
       "   'f1': 0.6469248291571754,\n",
       "   'auc': 0.7237492518786159},\n",
       "  'cotrain': {'recall': 0.5720524017467249,\n",
       "   'precision': 0.4763636363636364,\n",
       "   'f1': 0.5198412698412699,\n",
       "   'auc': 0.6032850841220935},\n",
       "  'ensemble': {'recall': 0.5458515283842795,\n",
       "   'precision': 0.5555555555555556,\n",
       "   'f1': 0.5506607929515418,\n",
       "   'auc': 0.6460222108926474},\n",
       "  'pretrained': {'recall': 0.5458515283842795,\n",
       "   'precision': 0.5555555555555556,\n",
       "   'f1': 0.5506607929515418,\n",
       "   'auc': 0.6460222108926474}},\n",
       " 0.2: {'self': {'recall': 0.6244541484716157,\n",
       "   'precision': 0.6777251184834123,\n",
       "   'f1': 0.65,\n",
       "   'auc': 0.725932657992153},\n",
       "  'cotrain': {'recall': 0.5589519650655022,\n",
       "   'precision': 0.4980544747081712,\n",
       "   'f1': 0.5267489711934157,\n",
       "   'auc': 0.6157703987764059},\n",
       "  'ensemble': {'recall': 0.5895196506550219,\n",
       "   'precision': 0.5895196506550219,\n",
       "   'f1': 0.5895196506550219,\n",
       "   'auc': 0.6754704852259882},\n",
       "  'pretrained': {'recall': 0.5895196506550219,\n",
       "   'precision': 0.5895196506550219,\n",
       "   'f1': 0.5895196506550219,\n",
       "   'auc': 0.6754704852259882}},\n",
       " 0.3: {'self': {'recall': 0.6331877729257642,\n",
       "   'precision': 0.6712962962962963,\n",
       "   'f1': 0.651685393258427,\n",
       "   'auc': 0.7264923636202425},\n",
       "  'cotrain': {'recall': 0.5283842794759825,\n",
       "   'precision': 0.4782608695652174,\n",
       "   'f1': 0.5020746887966805,\n",
       "   'auc': 0.5966794493826613},\n",
       "  'ensemble': {'recall': 0.6069868995633187,\n",
       "   'precision': 0.5840336134453782,\n",
       "   'f1': 0.5952890792291221,\n",
       "   'auc': 0.6778589320151619},\n",
       "  'pretrained': {'recall': 0.6069868995633187,\n",
       "   'precision': 0.5840336134453782,\n",
       "   'f1': 0.5952890792291221,\n",
       "   'auc': 0.6778589320151619}},\n",
       " 0.4: {'self': {'recall': 0.62882096069869,\n",
       "   'precision': 0.6792452830188679,\n",
       "   'f1': 0.6530612244897959,\n",
       "   'auc': 0.7281160641056903},\n",
       "  'cotrain': {'recall': 0.4890829694323144,\n",
       "   'precision': 0.4890829694323144,\n",
       "   'f1': 0.4890829694323144,\n",
       "   'auc': 0.5960643273557511},\n",
       "  'ensemble': {'recall': 0.5676855895196506,\n",
       "   'precision': 0.6018518518518519,\n",
       "   'f1': 0.5842696629213483,\n",
       "   'auc': 0.6747057389222619},\n",
       "  'pretrained': {'recall': 0.5676855895196506,\n",
       "   'precision': 0.6018518518518519,\n",
       "   'f1': 0.5842696629213483,\n",
       "   'auc': 0.6747057389222619}},\n",
       " 0.5: {'self': {'recall': 0.6506550218340611,\n",
       "   'precision': 0.680365296803653,\n",
       "   'f1': 0.6651785714285714,\n",
       "   'auc': 0.736495023607386},\n",
       "  'cotrain': {'recall': 0.5545851528384279,\n",
       "   'precision': 0.516260162601626,\n",
       "   'f1': 0.5347368421052632,\n",
       "   'auc': 0.6262773479928181},\n",
       "  'ensemble': {'recall': 0.5633187772925764,\n",
       "   'precision': 0.589041095890411,\n",
       "   'f1': 0.5758928571428571,\n",
       "   'auc': 0.6674461906767449},\n",
       "  'pretrained': {'recall': 0.5633187772925764,\n",
       "   'precision': 0.589041095890411,\n",
       "   'f1': 0.5758928571428571,\n",
       "   'auc': 0.6674461906767449}}}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = X_mushroom_train.to_numpy()\n",
    "y_train = y_mushroom_train.to_numpy()\n",
    "X_test = X_mushroom_test.to_numpy()\n",
    "y_test = y_mushroom_test.to_numpy()\n",
    "\n",
    "semi_experiement(X_train, X_test, y_train, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
